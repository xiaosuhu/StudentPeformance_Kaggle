{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f638a16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:39.229061Z",
     "iopub.status.busy": "2023-06-09T03:40:39.228257Z",
     "iopub.status.idle": "2023-06-09T03:40:40.815671Z",
     "shell.execute_reply": "2023-06-09T03:40:40.814568Z"
    },
    "papermill": {
     "duration": 1.603625,
     "end_time": "2023-06-09T03:40:40.818835",
     "exception": false,
     "start_time": "2023-06-09T03:40:39.215210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda3/envs/ag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "import pickle\n",
    "import sys\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d64c9ec3",
   "metadata": {
    "papermill": {
     "duration": 0.009149,
     "end_time": "2023-06-09T03:40:40.837837",
     "exception": false,
     "start_time": "2023-06-09T03:40:40.828688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Train Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a54cf9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:40.858977Z",
     "iopub.status.busy": "2023-06-09T03:40:40.858226Z",
     "iopub.status.idle": "2023-06-09T03:40:40.867210Z",
     "shell.execute_reply": "2023-06-09T03:40:40.866361Z"
    },
    "papermill": {
     "duration": 0.02232,
     "end_time": "2023-06-09T03:40:40.869708",
     "exception": false,
     "start_time": "2023-06-09T03:40:40.847388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\"session_id\": 'int64',\n",
    "          \"index\": np.int16,\n",
    "          \"elapsed_time\": np.int32,\n",
    "          \"event_name\": 'category',\n",
    "          \"name\": 'category',\n",
    "          \"level\": np.int8,\n",
    "          \"page\": np.float16,\n",
    "          \"room_coor_x\": np.float16,\n",
    "          \"room_coor_y\": np.float16,\n",
    "          \"screen_coor_x\": np.float16,\n",
    "          \"screen_coor_y\": np.float16,\n",
    "          \"hover_duration\": np.float32,\n",
    "          \"text\": 'category',\n",
    "          \"fqid\": 'category',\n",
    "          \"room_fqid\": 'category',\n",
    "          \"text_fqid\": 'category',\n",
    "          \"fullscreen\": np.int8,\n",
    "          \"hq\": np.int8,\n",
    "          \"music\": np.int8,\n",
    "          \"level_group\": 'category'\n",
    "          }\n",
    "use_col = ['session_id', 'index', 'elapsed_time', 'event_name', 'name', 'level', 'page',\n",
    "           'room_coor_x', 'room_coor_y', 'hover_duration', 'text', 'fqid', 'room_fqid', 'text_fqid', 'level_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a73405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:40.891690Z",
     "iopub.status.busy": "2023-06-09T03:40:40.890920Z",
     "iopub.status.idle": "2023-06-09T03:40:42.498388Z",
     "shell.execute_reply": "2023-06-09T03:40:42.497415Z"
    },
    "papermill": {
     "duration": 1.621235,
     "end_time": "2023-06-09T03:40:42.500672",
     "exception": false,
     "start_time": "2023-06-09T03:40:40.879437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424116, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "      <th>session</th>\n",
       "      <th>q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "      <td>20090312433251036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312455206810_q1</td>\n",
       "      <td>1</td>\n",
       "      <td>20090312455206810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20090313091715820_q1</td>\n",
       "      <td>0</td>\n",
       "      <td>20090313091715820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20090313571836404_q1</td>\n",
       "      <td>1</td>\n",
       "      <td>20090313571836404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct            session  q\n",
       "0  20090312431273200_q1        1  20090312431273200  1\n",
       "1  20090312433251036_q1        0  20090312433251036  1\n",
       "2  20090312455206810_q1        1  20090312455206810  1\n",
       "3  20090313091715820_q1        0  20090313091715820  1\n",
       "4  20090313571836404_q1        1  20090313571836404  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_csv('./Data/train_labels.csv')\n",
    "targets['session'] = targets.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "targets['q'] = targets.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "print( targets.shape )\n",
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f19986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.523175Z",
     "iopub.status.busy": "2023-06-09T03:40:42.522331Z",
     "iopub.status.idle": "2023-06-09T03:40:42.680643Z",
     "shell.execute_reply": "2023-06-09T03:40:42.679525Z"
    },
    "papermill": {
     "duration": 0.172873,
     "end_time": "2023-06-09T03:40:42.683618",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.510745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv('./CatVersionData/feature_sort.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30542314",
   "metadata": {
    "papermill": {
     "duration": 0.009618,
     "end_time": "2023-06-09T03:40:42.703284",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.693666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927ad972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.725773Z",
     "iopub.status.busy": "2023-06-09T03:40:42.724783Z",
     "iopub.status.idle": "2023-06-09T03:40:42.731690Z",
     "shell.execute_reply": "2023-06-09T03:40:42.730900Z"
    },
    "papermill": {
     "duration": 0.020539,
     "end_time": "2023-06-09T03:40:42.734026",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.713487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delt_time_def(df):\n",
    "    df.sort_values(by=['session_id', 'elapsed_time'], inplace=True)\n",
    "    df['d_time'] = df['elapsed_time'].diff(1)\n",
    "    df['d_time'].fillna(0, inplace=True)\n",
    "    df['delt_time'] = df['d_time'].clip(0, 103000)\n",
    "    df['delt_time_next'] = df['delt_time'].shift(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980f14ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.756739Z",
     "iopub.status.busy": "2023-06-09T03:40:42.755598Z",
     "iopub.status.idle": "2023-06-09T03:40:42.772395Z",
     "shell.execute_reply": "2023-06-09T03:40:42.771211Z"
    },
    "papermill": {
     "duration": 0.031246,
     "end_time": "2023-06-09T03:40:42.775410",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.744164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineer(train, kol_f):\n",
    "    global kol_col, kol_col_max\n",
    "    kol_col = 9\n",
    "    kol_col_max = 11+kol_f*2\n",
    "    col = [i for i in range(0,kol_col_max)]\n",
    "    new_train = pd.DataFrame(index=train['session_id'].unique(), columns=col, dtype=np.float16)  \n",
    "    new_train[10] = new_train.index # \"session_id\"    \n",
    "\n",
    "    new_train[0] = train.groupby(['session_id'])['d_time'].quantile(q=0.3)\n",
    "    new_train[1] = train.groupby(['session_id'])['d_time'].quantile(q=0.8)\n",
    "    new_train[2] = train.groupby(['session_id'])['d_time'].quantile(q=0.5)\n",
    "    new_train[3] = train.groupby(['session_id'])['d_time'].quantile(q=0.65)\n",
    "    new_train[4] = train.groupby(['session_id'])['hover_duration'].agg('mean')\n",
    "    new_train[5] = train.groupby(['session_id'])['hover_duration'].agg('std')    \n",
    "    new_train[6] = new_train[10].apply(lambda x: int(str(x)[:2])).astype(np.uint8) # \"year\"\n",
    "    new_train[7] = new_train[10].apply(lambda x: int(str(x)[2:4])+1).astype(np.uint8) # \"month\"\n",
    "    new_train[8] = new_train[10].apply(lambda x: int(str(x)[4:6])).astype(np.uint8) # \"day\"\n",
    "    new_train[9] = new_train[10].apply(lambda x: int(str(x)[6:8])).astype(np.uint8) + new_train[10].apply(lambda x: int(str(x)[8:10])).astype(np.uint8)/60\n",
    "    new_train[10] = 0\n",
    "    new_train = new_train.fillna(-1)\n",
    "    \n",
    "    return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a7d3fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.798108Z",
     "iopub.status.busy": "2023-06-09T03:40:42.797681Z",
     "iopub.status.idle": "2023-06-09T03:40:42.809629Z",
     "shell.execute_reply": "2023-06-09T03:40:42.808710Z"
    },
    "papermill": {
     "duration": 0.026331,
     "end_time": "2023-06-09T03:40:42.811929",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.785598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_next_t(row_f, new_train, train, gran_1, gran_2, i):\n",
    "    global kol_col\n",
    "    kol_col +=1\n",
    "    col1 = row_f['col1']\n",
    "    val1 = row_f['val1']\n",
    "    maska = (train[col1] == val1)\n",
    "    if row_f['kol_col'] == 1:       \n",
    "        new_train[kol_col] = train[maska].groupby(['session_id'])['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska].groupby(['session_id'])['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska].groupby(['session_id'])['index'].count()          \n",
    "    elif row_f['kol_col'] == 2: \n",
    "        col2 = row_f['col2']\n",
    "        val2 = row_f['val2']\n",
    "        maska = maska & (train[col2] == val2)        \n",
    "        new_train[kol_col] = train[maska].groupby(['session_id'])['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska].groupby(['session_id'])['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska].groupby(['session_id'])['index'].count()\n",
    "    return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ff91bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.833679Z",
     "iopub.status.busy": "2023-06-09T03:40:42.833289Z",
     "iopub.status.idle": "2023-06-09T03:40:42.844768Z",
     "shell.execute_reply": "2023-06-09T03:40:42.843807Z"
    },
    "papermill": {
     "duration": 0.025038,
     "end_time": "2023-06-09T03:40:42.847178",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.822140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_next_t_otvet(row_f, new_train, train, gran_1, gran_2, i):\n",
    "    global kol_col\n",
    "    kol_col +=1\n",
    "    col1 = row_f['col1']\n",
    "    val1 = row_f['val1']\n",
    "    maska = (train[col1] == val1)\n",
    "    if row_f['kol_col'] == 1:      \n",
    "        new_train[kol_col] = train[maska]['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['index'].count()          \n",
    "    elif row_f['kol_col'] == 2: \n",
    "        col2 = row_f['col2']\n",
    "        val2 = row_f['val2']\n",
    "        maska = maska & (train[col2] == val2)        \n",
    "        new_train[kol_col] = train[maska]['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['index'].count()\n",
    "    return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb42f1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.868991Z",
     "iopub.status.busy": "2023-06-09T03:40:42.868582Z",
     "iopub.status.idle": "2023-06-09T03:40:42.879217Z",
     "shell.execute_reply": "2023-06-09T03:40:42.878273Z"
    },
    "papermill": {
     "duration": 0.024469,
     "end_time": "2023-06-09T03:40:42.881703",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.857234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experiment_feature_next_t_otvet(row_f, new_train, train, gran_1, gran_2, i):\n",
    "    global kol_col\n",
    "    kol_col +=1\n",
    "    if row_f['kol_col'] == 1: \n",
    "        maska = train[row_f['col1']] == row_f['val1']\n",
    "        new_train[kol_col] = train[maska]['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['index'].count()          \n",
    "    elif row_f['kol_col'] == 2: \n",
    "        col2 = row_f['col2']\n",
    "        val2 = row_f['val2']\n",
    "        maska = (train[col1] == val1) & (train[col2] == val2)        \n",
    "        new_train[kol_col] = train[maska]['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['index'].count()\n",
    "    return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37a5e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.904121Z",
     "iopub.status.busy": "2023-06-09T03:40:42.903176Z",
     "iopub.status.idle": "2023-06-09T03:40:42.911574Z",
     "shell.execute_reply": "2023-06-09T03:40:42.910759Z"
    },
    "papermill": {
     "duration": 0.022073,
     "end_time": "2023-06-09T03:40:42.913858",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.891785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_quest_otvet(new_train, train, quest, kol_f):\n",
    "    global kol_col\n",
    "    kol_col = 9\n",
    "    g1 = 0.7 \n",
    "    g2 = 0.3 \n",
    "\n",
    "    feature_q = feature_df[feature_df['quest'] == quest].copy()\n",
    "    feature_q.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gran1 = round(kol_f * g1)\n",
    "    gran2 = round(kol_f * g2)    \n",
    "    for i in range(0, kol_f):         \n",
    "        row_f = feature_q.loc[i]\n",
    "        new_train = feature_next_t_otvet(row_f, new_train, train, i < gran1, i <  gran2, i) \n",
    "    col = [i for i in range(0,kol_col+1)]\n",
    "    return new_train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7ce1f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.936079Z",
     "iopub.status.busy": "2023-06-09T03:40:42.935270Z",
     "iopub.status.idle": "2023-06-09T03:40:42.941759Z",
     "shell.execute_reply": "2023-06-09T03:40:42.940942Z"
    },
    "papermill": {
     "duration": 0.02019,
     "end_time": "2023-06-09T03:40:42.944055",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.923865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineer_new(new_train, train, feature_q, kol_f):\n",
    "    g1 = 0.7 \n",
    "    g2 = 0.3 \n",
    "    gran1 = round(kol_f * g1)\n",
    "    gran2 = round(kol_f * g2)    \n",
    "    for i in range(0, kol_f): \n",
    "        row_f = feature_q.loc[i]       \n",
    "        new_train = feature_next_t(row_f, new_train, train, i < gran1, i <  gran2, i)         \n",
    "    return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d5ca27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.966386Z",
     "iopub.status.busy": "2023-06-09T03:40:42.965638Z",
     "iopub.status.idle": "2023-06-09T03:40:42.972769Z",
     "shell.execute_reply": "2023-06-09T03:40:42.971945Z"
    },
    "papermill": {
     "duration": 0.021014,
     "end_time": "2023-06-09T03:40:42.975183",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.954169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_quest(new_train, train, quest, kol_f):\n",
    "    global kol_col\n",
    "    kol_col = 9\n",
    "    feature_q = feature_df[feature_df['quest'] == quest].copy()\n",
    "    feature_q.reset_index(drop=True, inplace=True)\n",
    "    new_train = feature_engineer_new(new_train, train, feature_q, kol_f)\n",
    "    col = [i for i in range(0,kol_col+1)]\n",
    "    return new_train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd7b7f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:42.997280Z",
     "iopub.status.busy": "2023-06-09T03:40:42.996608Z",
     "iopub.status.idle": "2023-06-09T03:40:43.005786Z",
     "shell.execute_reply": "2023-06-09T03:40:43.004940Z"
    },
    "papermill": {
     "duration": 0.023079,
     "end_time": "2023-06-09T03:40:43.008244",
     "exception": false,
     "start_time": "2023-06-09T03:40:42.985165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(old_train, quests, models, list_kol_f):\n",
    "    \n",
    "    kol_quest = len(quests)\n",
    "    # ITERATE THRU QUESTIONS\n",
    "    for q in quests:\n",
    "        print('### quest ', q, end='')\n",
    "        new_train = feature_engineer(old_train, list_kol_f[q])\n",
    "        train_x = feature_quest(new_train, old_train, q, list_kol_f[q])\n",
    "        print (' ---- ', 'train_q.shape = ', train_x.shape)\n",
    "           \n",
    "        # TRAIN DATA\n",
    "        train_users = train_x.index.values\n",
    "        train_y = targets.loc[targets.q==q].set_index('session').loc[train_users]\n",
    "        \n",
    "        train_ag = TabularDataset(train_x.join(train_y, how = 'left'))\n",
    "\n",
    "        # TRAIN MODEL \n",
    "        predictor = TabularPredictor(label='correct').fit(train_ag.drop(['session_id','q'], axis = 1),  num_gpus=1,)\n",
    "\n",
    "        # SAVE MODEL, PREDICT VALID OOF\n",
    "        models[f'{q}'] = predictor\n",
    "    print('***')\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e8888b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:43.030824Z",
     "iopub.status.busy": "2023-06-09T03:40:43.030104Z",
     "iopub.status.idle": "2023-06-09T03:40:43.034765Z",
     "shell.execute_reply": "2023-06-09T03:40:43.033869Z"
    },
    "papermill": {
     "duration": 0.019119,
     "end_time": "2023-06-09T03:40:43.037441",
     "exception": false,
     "start_time": "2023-06-09T03:40:43.018322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "best_threshold = 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48777192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:43.059951Z",
     "iopub.status.busy": "2023-06-09T03:40:43.059204Z",
     "iopub.status.idle": "2023-06-09T03:40:43.064701Z",
     "shell.execute_reply": "2023-06-09T03:40:43.063938Z"
    },
    "papermill": {
     "duration": 0.019279,
     "end_time": "2023-06-09T03:40:43.066958",
     "exception": false,
     "start_time": "2023-06-09T03:40:43.047679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_kol_f = {\n",
    "    1:140,3:110,\n",
    "    4:120, 5:220, 6:130, 7:110, 8:110, 9:100, 10:140, 11:120,\n",
    "    14: 160, 15:160, 16:130, 17:140             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d087c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:40:43.088814Z",
     "iopub.status.busy": "2023-06-09T03:40:43.088120Z",
     "iopub.status.idle": "2023-06-09T03:42:10.232769Z",
     "shell.execute_reply": "2023-06-09T03:42:10.231605Z"
    },
    "papermill": {
     "duration": 87.158188,
     "end_time": "2023-06-09T03:42:10.235162",
     "exception": false,
     "start_time": "2023-06-09T03:40:43.076974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_141511/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (23562 samples, 54.98 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_141511/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23562\n",
      "Train Data Columns: 290\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28952.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.07 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23562, 290)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 269 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  21 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 269 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  21 | ['6', '7', '8', '12', '15', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t290 features in original data used to generate 290 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.07 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21205, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7136\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7136\t = Validation score   (accuracy)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7611\t = Validation score   (accuracy)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7624\t = Validation score   (accuracy)\n",
      "\t2.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7637\t = Validation score   (accuracy)\n",
      "\t6.69s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7599\t = Validation score   (accuracy)\n",
      "\t7.59s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t0.762\t = Validation score   (accuracy)\n",
      "\t5.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7582\t = Validation score   (accuracy)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7535\t = Validation score   (accuracy)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.7658\t = Validation score   (accuracy)\n",
      "\t12.86s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7667\t = Validation score   (accuracy)\n",
      "\t1.86s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7569\t = Validation score   (accuracy)\n",
      "\t7.46s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.765\t = Validation score   (accuracy)\n",
      "\t9.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7696\t = Validation score   (accuracy)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 62.53s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_141511/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_141620/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_141620/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23562\n",
      "Train Data Columns: 230\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    26094.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 42.77 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23562, 230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 15): ['215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 201 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  14 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 201 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  14 | ['6', '7', '8', '12', '15', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t215 features in original data used to generate 215 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.94 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21205, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9275\t = Validation score   (accuracy)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9275\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.9351\t = Validation score   (accuracy)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.9347\t = Validation score   (accuracy)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9338\t = Validation score   (accuracy)\n",
      "\t6.68s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9338\t = Validation score   (accuracy)\n",
      "\t5.45s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 7405.5625 Total: 10001.3125\n",
      "\t0.9338\t = Validation score   (accuracy)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9342\t = Validation score   (accuracy)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9338\t = Validation score   (accuracy)\n",
      "\t0.97s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 7: early stopping\n",
      "\t0.9342\t = Validation score   (accuracy)\n",
      "\t10.32s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9351\t = Validation score   (accuracy)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9338\t = Validation score   (accuracy)\n",
      "\t6.4s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.9338\t = Validation score   (accuracy)\n",
      "\t3.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9351\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 41.75s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_141620/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n"
     ]
    }
   ],
   "source": [
    "df0_4 = pd.read_csv('./CatVersionData/train_0_4t.csv', dtype=dtypes) \n",
    "kol_lvl = (df0_4 .groupby(['session_id'])['level'].agg('nunique') < 5)\n",
    "list_session = kol_lvl[kol_lvl].index\n",
    "df0_4  = df0_4 [~df0_4 ['session_id'].isin(list_session)]\n",
    "df0_4 = delt_time_def(df0_4)\n",
    "\n",
    "quests_0_4 = [1, 3] \n",
    "# list_kol_f = {1:140,3:110}\n",
    "\n",
    "models = create_model(df0_4, quests_0_4, models, list_kol_f)\n",
    "del df0_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16bd6956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:42:10.257926Z",
     "iopub.status.busy": "2023-06-09T03:42:10.257517Z",
     "iopub.status.idle": "2023-06-09T03:49:38.417719Z",
     "shell.execute_reply": "2023-06-09T03:49:38.416800Z"
    },
    "papermill": {
     "duration": 448.186573,
     "end_time": "2023-06-09T03:49:38.432208",
     "exception": false,
     "start_time": "2023-06-09T03:42:10.245635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_144031/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_144031/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23561\n",
      "Train Data Columns: 250\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25594.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 46.53 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23561, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 219 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  31 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 216 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])       :  31 | ['6', '7', '8', '12', '15', ...]\n",
      "\t\t('int', ['bool']) :   3 | ['49', '50', '51']\n",
      "\t0.4s = Fit runtime\n",
      "\t250 features in original data used to generate 250 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.04 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21204, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7781\t = Validation score   (accuracy)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7781\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.8188\t = Validation score   (accuracy)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.8193\t = Validation score   (accuracy)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.8116\t = Validation score   (accuracy)\n",
      "\t9.19s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.8133\t = Validation score   (accuracy)\n",
      "\t7.88s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t0.8163\t = Validation score   (accuracy)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.8104\t = Validation score   (accuracy)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.8104\t = Validation score   (accuracy)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.8116\t = Validation score   (accuracy)\n",
      "\t9.96s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8184\t = Validation score   (accuracy)\n",
      "\t1.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8176\t = Validation score   (accuracy)\n",
      "\t8.26s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.8201\t = Validation score   (accuracy)\n",
      "\t5.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.8239\t = Validation score   (accuracy)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 52.31s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_144031/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_144139/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (23561 samples, 85.14 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_144139/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23561\n",
      "Train Data Columns: 450\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25398.1 MB\n",
      "\tTrain Data (Original)  Memory Usage: 84.23 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23561, 450)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 391 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  59 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 391 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  59 | ['6', '7', '8', '12', '15', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t450 features in original data used to generate 450 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 84.23 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21204, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.5817\t = Validation score   (accuracy)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.5817\t = Validation score   (accuracy)\n",
      "\t0.17s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6759\t = Validation score   (accuracy)\n",
      "\t3.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6793\t = Validation score   (accuracy)\n",
      "\t3.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.6712\t = Validation score   (accuracy)\n",
      "\t8.87s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6627\t = Validation score   (accuracy)\n",
      "\t11.07s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t0.6742\t = Validation score   (accuracy)\n",
      "\t6.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.6623\t = Validation score   (accuracy)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.6623\t = Validation score   (accuracy)\n",
      "\t2.44s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 2: early stopping\n",
      "\t0.6542\t = Validation score   (accuracy)\n",
      "\t10.46s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.6754\t = Validation score   (accuracy)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6725\t = Validation score   (accuracy)\n",
      "\t8.68s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6712\t = Validation score   (accuracy)\n",
      "\t9.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.6848\t = Validation score   (accuracy)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 73.71s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_144139/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  6"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_144306/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (23561 samples, 51.21 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_144306/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23561\n",
      "Train Data Columns: 270\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25469.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 50.3 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23561, 270)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 234 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  36 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 234 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  36 | ['6', '7', '8', '12', '15', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t270 features in original data used to generate 270 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.3 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21204, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7594\t = Validation score   (accuracy)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7594\t = Validation score   (accuracy)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7968\t = Validation score   (accuracy)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7989\t = Validation score   (accuracy)\n",
      "\t8.87s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7959\t = Validation score   (accuracy)\n",
      "\t8.72s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t0.7989\t = Validation score   (accuracy)\n",
      "\t3.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7917\t = Validation score   (accuracy)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7913\t = Validation score   (accuracy)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 5: early stopping\n",
      "\t0.7955\t = Validation score   (accuracy)\n",
      "\t10.02s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7917\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7993\t = Validation score   (accuracy)\n",
      "\t7.46s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7959\t = Validation score   (accuracy)\n",
      "\t5.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.8014\t = Validation score   (accuracy)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 54.97s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_144306/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_144413/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_144413/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23561\n",
      "Train Data Columns: 230\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25455.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 42.76 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23561, 230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 201 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  29 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 201 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  29 | ['6', '7', '8', '12', '15', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t230 features in original data used to generate 230 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.76 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.4s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21204, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.6979\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.6979\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7501\t = Validation score   (accuracy)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7501\t = Validation score   (accuracy)\n",
      "\t1.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7442\t = Validation score   (accuracy)\n",
      "\t7.91s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7454\t = Validation score   (accuracy)\n",
      "\t8.61s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t0.751\t = Validation score   (accuracy)\n",
      "\t5.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7429\t = Validation score   (accuracy)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7442\t = Validation score   (accuracy)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.7518\t = Validation score   (accuracy)\n",
      "\t10.13s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7476\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7493\t = Validation score   (accuracy)\n",
      "\t7.84s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7471\t = Validation score   (accuracy)\n",
      "\t5.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7531\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 56.39s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_144413/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  8"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_144522/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_144522/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23561\n",
      "Train Data Columns: 230\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25388.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 42.76 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23561, 230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 202 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  28 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 198 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])       :  28 | ['6', '7', '8', '12', '15', ...]\n",
      "\t\t('int', ['bool']) :   4 | ['41', '42', '45', '48']\n",
      "\t0.4s = Fit runtime\n",
      "\t230 features in original data used to generate 230 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.1 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21204, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.5605\t = Validation score   (accuracy)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.5609\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6423\t = Validation score   (accuracy)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6483\t = Validation score   (accuracy)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.6305\t = Validation score   (accuracy)\n",
      "\t6.88s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6322\t = Validation score   (accuracy)\n",
      "\t9.5s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 7393.625 Total: 10001.3125\n",
      "\t0.6322\t = Validation score   (accuracy)\n",
      "\t2.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.6283\t = Validation score   (accuracy)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.6262\t = Validation score   (accuracy)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.6262\t = Validation score   (accuracy)\n",
      "\t9.86s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.6411\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6423\t = Validation score   (accuracy)\n",
      "\t7.68s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6415\t = Validation score   (accuracy)\n",
      "\t5.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.6496\t = Validation score   (accuracy)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 53.01s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_144522/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_144627/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_144627/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23561\n",
      "Train Data Columns: 210\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25240.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 38.99 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23561, 210)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 183 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  27 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 183 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  27 | ['6', '7', '8', '12', '15', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t210 features in original data used to generate 210 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.99 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21204, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7111\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7111\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7484\t = Validation score   (accuracy)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7552\t = Validation score   (accuracy)\n",
      "\t1.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7501\t = Validation score   (accuracy)\n",
      "\t7.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7484\t = Validation score   (accuracy)\n",
      "\t8.0s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 7000.75 Total: 10001.3125\n",
      "\t0.7463\t = Validation score   (accuracy)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7467\t = Validation score   (accuracy)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7497\t = Validation score   (accuracy)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 7: early stopping\n",
      "\t0.7497\t = Validation score   (accuracy)\n",
      "\t10.18s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.745\t = Validation score   (accuracy)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7501\t = Validation score   (accuracy)\n",
      "\t6.96s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.7488\t = Validation score   (accuracy)\n",
      "\t6.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7556\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_144627/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_144731/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (23561 samples, 54.98 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_144731/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23561\n",
      "Train Data Columns: 290\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25220.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.07 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23561, 290)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['287', '288', '289']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 248 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  39 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 245 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])       :  39 | ['6', '7', '8', '12', '15', ...]\n",
      "\t\t('int', ['bool']) :   3 | ['281', '282', '283']\n",
      "\t0.4s = Fit runtime\n",
      "\t287 features in original data used to generate 287 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 53.01 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21204, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.5541\t = Validation score   (accuracy)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.5541\t = Validation score   (accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6254\t = Validation score   (accuracy)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6296\t = Validation score   (accuracy)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.619\t = Validation score   (accuracy)\n",
      "\t6.74s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6313\t = Validation score   (accuracy)\n",
      "\t8.94s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 7368.375 Total: 10001.3125\n",
      "\t0.6296\t = Validation score   (accuracy)\n",
      "\t8.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.6143\t = Validation score   (accuracy)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.619\t = Validation score   (accuracy)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t0.4947\t = Validation score   (accuracy)\n",
      "\t9.12s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.6279\t = Validation score   (accuracy)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6385\t = Validation score   (accuracy)\n",
      "\t8.99s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6313\t = Validation score   (accuracy)\n",
      "\t5.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.6428\t = Validation score   (accuracy)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 59.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_144731/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  11"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230619_144843/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230619_144843/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #82-Ubuntu SMP Tue Jun 6 23:10:23 UTC 2023\n",
      "Train Data Rows:    23561\n",
      "Train Data Columns: 250\n",
      "Label Column: correct\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25174.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 46.53 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----  train_q.shape =  (23561, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 223 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  27 | ['6', '7', '8', '12', '15', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 223 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', [])   :  27 | ['6', '7', '8', '12', '15', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t250 features in original data used to generate 250 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.53 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 21204, Val Rows: 2357\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.6211\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.6207\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.675\t = Validation score   (accuracy)\n",
      "\t1.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.6733\t = Validation score   (accuracy)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.6691\t = Validation score   (accuracy)\n",
      "\t7.33s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6746\t = Validation score   (accuracy)\n",
      "\t9.66s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 7366.1875 Total: 10001.3125\n",
      "\t0.6763\t = Validation score   (accuracy)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.6665\t = Validation score   (accuracy)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.6665\t = Validation score   (accuracy)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.6644\t = Validation score   (accuracy)\n",
      "\t9.08s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.6737\t = Validation score   (accuracy)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6805\t = Validation score   (accuracy)\n",
      "\t7.27s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.678\t = Validation score   (accuracy)\n",
      "\t4.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.6822\t = Validation score   (accuracy)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 51.57s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230619_144843/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n"
     ]
    }
   ],
   "source": [
    "df5_12 = pd.read_csv('./CatVersionData/train_5_12t.csv', dtype=dtypes)\n",
    "kol_lvl = (df5_12.groupby(['session_id'])['level'].agg('nunique') < 8)\n",
    "list_session = kol_lvl[kol_lvl].index\n",
    "df5_12 = df5_12[~df5_12['session_id'].isin(list_session)]\n",
    "df5_12 = delt_time_def(df5_12)\n",
    "quests_5_12 = [4, 5, 6, 7, 8, 9, 10, 11] \n",
    "\n",
    "# list_kol_f = {4:110, 5:220, 6:120, 7:110, 8:110, 9:100, 10:140, 11:120}\n",
    "\n",
    "models = create_model(df5_12, quests_5_12, models, list_kol_f)\n",
    "del df5_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b75d0877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:49:38.459132Z",
     "iopub.status.busy": "2023-06-09T03:49:38.458288Z",
     "iopub.status.idle": "2023-06-09T03:54:51.909389Z",
     "shell.execute_reply": "2023-06-09T03:54:51.907976Z"
    },
    "papermill": {
     "duration": 313.467992,
     "end_time": "2023-06-09T03:54:51.912359",
     "exception": false,
     "start_time": "2023-06-09T03:49:38.444367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### quest  14 ----  train_q.shape =  (22986, 330)\n",
      "### quest  15 ----  train_q.shape =  (22986, 330)\n",
      "### quest  16 ----  train_q.shape =  (22986, 270)\n",
      "### quest  17 ----  train_q.shape =  (22986, 290)\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "df13_22 = pd.read_csv('./CatVersionData/train_13_22t.csv', dtype=dtypes) \n",
    "kol_lvl = (df13_22 .groupby(['session_id'])['level'].agg('nunique') < 10)\n",
    "list_session = kol_lvl[kol_lvl].index\n",
    "df13_22  = df13_22 [~df13_22 ['session_id'].isin(list_session)]\n",
    "df13_22 = delt_time_def(df13_22)\n",
    "\n",
    "quests_13_22 = [14, 15, 16, 17] \n",
    "# list_kol_f = {14: 160, 15:160, 16:105, 17:140}\n",
    "\n",
    "models = create_model(df13_22, quests_13_22, models, list_kol_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3822caa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:54:51.941051Z",
     "iopub.status.busy": "2023-06-09T03:54:51.939939Z",
     "iopub.status.idle": "2023-06-09T03:54:51.944954Z",
     "shell.execute_reply": "2023-06-09T03:54:51.944069Z"
    },
    "papermill": {
     "duration": 0.022067,
     "end_time": "2023-06-09T03:54:51.947378",
     "exception": false,
     "start_time": "2023-06-09T03:54:51.925311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Saving a Model\n",
    "for q in quests_0_4 + quests_5_12 + quests_13_22:\n",
    "    models[q].save_model(f'cat_model_{q}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f26a0493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:54:51.975175Z",
     "iopub.status.busy": "2023-06-09T03:54:51.974387Z",
     "iopub.status.idle": "2023-06-09T03:54:51.978737Z",
     "shell.execute_reply": "2023-06-09T03:54:51.977919Z"
    },
    "papermill": {
     "duration": 0.020395,
     "end_time": "2023-06-09T03:54:51.980853",
     "exception": false,
     "start_time": "2023-06-09T03:54:51.960458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model Reading\n",
    "dir = '/kaggle/input/catbust/'\n",
    "for q in quests_0_4 + quests_5_12 + quests_13_22:\n",
    "     models[q] = CatBoostClassifier().load_model(dir+f'cat_model_{q}.bin')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb780281",
   "metadata": {
    "papermill": {
     "duration": 0.012106,
     "end_time": "2023-06-09T03:54:52.005241",
     "exception": false,
     "start_time": "2023-06-09T03:54:51.993135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Infer Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fca7c5a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:54:52.032791Z",
     "iopub.status.busy": "2023-06-09T03:54:52.031515Z",
     "iopub.status.idle": "2023-06-09T03:54:52.070092Z",
     "shell.execute_reply": "2023-06-09T03:54:52.069118Z"
    },
    "papermill": {
     "duration": 0.055104,
     "end_time": "2023-06-09T03:54:52.072758",
     "exception": false,
     "start_time": "2023-06-09T03:54:52.017654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jo_wilder\n",
    "\n",
    "try:\n",
    "    jo_wilder.make_env.__called__ = False\n",
    "    env.__called__ = False\n",
    "    type(env)._state = type(type(env)._state).__dict__['INIT']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env = jo_wilder.make_env()\n",
    "iter_test = env.iter_test()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d245ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:54:52.099944Z",
     "iopub.status.busy": "2023-06-09T03:54:52.098941Z",
     "iopub.status.idle": "2023-06-09T03:54:52.103915Z",
     "shell.execute_reply": "2023-06-09T03:54:52.102999Z"
    },
    "papermill": {
     "duration": 0.020917,
     "end_time": "2023-06-09T03:54:52.106235",
     "exception": false,
     "start_time": "2023-06-09T03:54:52.085318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0b22c67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:54:52.134090Z",
     "iopub.status.busy": "2023-06-09T03:54:52.133217Z",
     "iopub.status.idle": "2023-06-09T03:55:04.452914Z",
     "shell.execute_reply": "2023-06-09T03:55:04.451920Z"
    },
    "papermill": {
     "duration": 12.336681,
     "end_time": "2023-06-09T03:55:04.455695",
     "exception": false,
     "start_time": "2023-06-09T03:54:52.119014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "g_end4 = 0\n",
    "g_end5 = 0\n",
    "\n",
    "list_q = {'0-4':quests_0_4, '5-12':quests_5_12, '13-22':quests_13_22}\n",
    "for (test, sam_sub) in iter_test:\n",
    "    sam_sub['question'] = [int(label.split('_')[1][1:]) for label in sam_sub['session_id']]    \n",
    "    grp = test.level_group.values[0]   \n",
    "    sam_sub['correct'] = 1\n",
    "    sam_sub.loc[sam_sub.question.isin([5, 8, 10, 13, 15]), 'correct'] = 0  \n",
    "    old_train = delt_time_def(test[test.level_group == grp])\n",
    "       \n",
    "    for q in list_q[grp]:\n",
    "        \n",
    "        start4 = time.time()\n",
    "        new_train = feature_engineer(old_train, list_kol_f[q])\n",
    "        new_train = feature_quest_otvet(new_train, old_train, q, list_kol_f[q])\n",
    "#         new_train = feature_quest(new_train, old_train, q, kol_f)\n",
    "        \n",
    "        end4 = time.time() - start4\n",
    "        g_end4 += end4\n",
    "        \n",
    "        start5 = time.time()        \n",
    "        \n",
    "        clf = models[f'{q}']\n",
    "        p = clf.predict_proba(new_train.astype('float32'))[:,1]        \n",
    "        \n",
    "        end5 = time.time() - start5\n",
    "        g_end5 += end5\n",
    "             \n",
    "        \n",
    "        mask = sam_sub.question == q \n",
    "        x = int(p[0]>best_threshold)\n",
    "        sam_sub.loc[mask,'correct'] = x      \n",
    "        \n",
    "        \n",
    "    sam_sub = sam_sub[['session_id', 'correct']]      \n",
    "    env.predict(sam_sub)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7af7113",
   "metadata": {
    "papermill": {
     "duration": 0.012715,
     "end_time": "2023-06-09T03:55:04.481961",
     "exception": false,
     "start_time": "2023-06-09T03:55:04.469246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EDA submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9a741d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:55:04.509675Z",
     "iopub.status.busy": "2023-06-09T03:55:04.508935Z",
     "iopub.status.idle": "2023-06-09T03:55:04.512968Z",
     "shell.execute_reply": "2023-06-09T03:55:04.512099Z"
    },
    "papermill": {
     "duration": 0.020568,
     "end_time": "2023-06-09T03:55:04.515293",
     "exception": false,
     "start_time": "2023-06-09T03:55:04.494725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('submission.csv')\n",
    "# print( df.shape )\n",
    "# df.head(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 877.306385,
   "end_time": "2023-06-09T03:55:05.455065",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-09T03:40:28.148680",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
